{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be0d460-7bf2-4aaf-8847-68f81c1c3fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from pycocotools.coco import COCO\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import math\n",
    "\n",
    "\n",
    "COCO_CLASSES = 80\n",
    "ANCHORS = [\n",
    "    [(116, 90), (156, 198), (373, 326)],  # Layer 82\n",
    "    [(30, 61), (62, 45), (59, 119)],      # Layer 94\n",
    "    [(10, 13), (16, 30), (33, 23)]        # Layer 106\n",
    "]\n",
    "IMAGE_SIZE = 416\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1):\n",
    "        super().__init__()\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.leaky = nn.LeakyReLU(0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.leaky(self.bn(self.conv(x)))\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.block = nn.Sequential(\n",
    "            ConvBlock(channels, channels // 2, 1),\n",
    "            ConvBlock(channels // 2, channels, 3)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class Darknet53(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.conv1 = ConvBlock(3, 32, 3)\n",
    " \n",
    "        self.conv2 = ConvBlock(32, 64, 3, stride=2)\n",
    "        self.res1 = nn.Sequential(ResidualBlock(64))\n",
    "        \n",
    "        self.conv3 = ConvBlock(64, 128, 3, stride=2)\n",
    "        self.res2 = nn.Sequential(*[ResidualBlock(128) for _ in range(2)])\n",
    "\n",
    "        self.conv4 = ConvBlock(128, 256, 3, stride=2)\n",
    "        self.res3 = nn.Sequential(*[ResidualBlock(256) for _ in range(8)])\n",
    "\n",
    "        self.conv5 = ConvBlock(256, 512, 3, stride=2)\n",
    "        self.res4 = nn.Sequential(*[ResidualBlock(512) for _ in range(8)])\n",
    "\n",
    "        self.conv6 = ConvBlock(512, 1024, 3, stride=2)\n",
    "        self.res5 = nn.Sequential(*[ResidualBlock(1024) for _ in range(4)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.res1(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.res2(x)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = self.res3(x)\n",
    "        features.append(x)  \n",
    "        \n",
    "        x = self.conv5(x)\n",
    "        x = self.res4(x)\n",
    "        features.append(x) \n",
    "        \n",
    "        x = self.conv6(x)\n",
    "        x = self.res5(x)\n",
    "        features.append(x) \n",
    "        \n",
    "        return features\n",
    "\n",
    "class DetectionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = ConvBlock(in_channels, out_channels, 1)\n",
    "        self.conv2 = ConvBlock(out_channels, out_channels * 2, 3)\n",
    "        self.conv3 = ConvBlock(out_channels * 2, out_channels, 1)\n",
    "        self.conv4 = ConvBlock(out_channels, out_channels * 2, 3)\n",
    "        self.conv5 = ConvBlock(out_channels * 2, out_channels, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        return x\n",
    "\n",
    "class YOLOv3(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.backbone = Darknet53()\n",
    "        \n",
    "        # Detection layers\n",
    "        self.detect1 = DetectionBlock(1024, 512)\n",
    "        self.detect2 = DetectionBlock(512, 256)\n",
    "        self.detect3 = DetectionBlock(256, 128)\n",
    "        \n",
    "        # Prediction conv layers (output: [batch, anchors * (5 + num_classes), grid, grid])\n",
    "        self.pred1 = nn.Conv2d(512, 3 * (5 + num_classes), 1)\n",
    "        self.pred2 = nn.Conv2d(256, 3 * (5 + num_classes), 1)\n",
    "        self.pred3 = nn.Conv2d(128, 3 * (5 + num_classes), 1)\n",
    "        \n",
    "        # Upsampling layers\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        \n",
    "        # Additional conv layers for feature map processing\n",
    "        self.conv1 = ConvBlock(512, 256, 1)\n",
    "        self.conv2 = ConvBlock(256, 128, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        f1, f2, f3 = self.backbone(x)  # Large to small feature maps\n",
    "        \n",
    "        # First detection branch\n",
    "        x1 = self.detect1(f3)\n",
    "        p1 = self.pred1(x1)\n",
    "        \n",
    "        # Second detection branch\n",
    "        x1 = self.conv1(x1)\n",
    "        x1 = self.upsample(x1)\n",
    "        x2 = torch.cat([x1, f2], dim=1)\n",
    "        x2 = self.detect2(x2)\n",
    "        p2 = self.pred2(x2)\n",
    "        \n",
    "        # Third detection branch\n",
    "        x2 = self.conv2(x2)\n",
    "        x2 = self.upsample(x2)\n",
    "        x3 = torch.cat([x2, f1], dim=1)\n",
    "        x3 = self.detect3(x3)\n",
    "        p3 = self.pred3(x3)\n",
    "        \n",
    "        return p1, p2, p3\n",
    "\n",
    "class YOLOLoss(nn.Module):\n",
    "    def __init__(self, anchors, num_classes, img_size):\n",
    "        super().__init__()\n",
    "        self.anchors = anchors\n",
    "        self.num_classes = num_classes\n",
    "        self.img_size = img_size\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.bce = nn.BCELoss()\n",
    "        self.ignore_thres = 0.5\n",
    "        self.obj_scale = 1\n",
    "        self.noobj_scale = 100\n",
    "        self.metrics = {}\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        device = predictions[0].device\n",
    "        total_loss = 0\n",
    "        \n",
    "        for i, pred in enumerate(predictions):\n",
    "            batch_size = pred.size(0)\n",
    "            grid_size = pred.size(2)\n",
    "            \n",
    "            # Transform predictions\n",
    "            prediction = pred.view(batch_size, 3, 5 + self.num_classes, grid_size, grid_size)\n",
    "            prediction = prediction.permute(0, 1, 3, 4, 2).contiguous()\n",
    "            \n",
    "            # Get outputs\n",
    "            x = torch.sigmoid(prediction[..., 0])\n",
    "            y = torch.sigmoid(prediction[..., 1])\n",
    "            w = prediction[..., 2]  \n",
    "            h = prediction[..., 3]  \n",
    "            pred_conf = torch.sigmoid(prediction[..., 4])  # Object confidence\n",
    "            pred_cls = torch.sigmoid(prediction[..., 5:])  # Class predictions\n",
    "            \n",
    "            # Calculate offsets for each grid\n",
    "            grid_x = torch.arange(grid_size, device=device).repeat(grid_size, 1).view([1, 1, grid_size, grid_size])\n",
    "            grid_y = torch.arange(grid_size, device=device).repeat(grid_size, 1).t().view([1, 1, grid_size, grid_size])\n",
    "            scaled_anchors = torch.tensor([(a_w / self.img_size, a_h / self.img_size) \n",
    "                                         for a_w, a_h in self.anchors[i]], device=device)\n",
    "            anchor_w = scaled_anchors[:, 0:1].view((1, 3, 1, 1))\n",
    "            anchor_h = scaled_anchors[:, 1:2].view((1, 3, 1, 1))\n",
    "            \n",
    "            # Add offset and scale with anchors\n",
    "            pred_boxes = torch.zeros_like(prediction[..., :4])\n",
    "            pred_boxes[..., 0] = x + grid_x\n",
    "            pred_boxes[..., 1] = y + grid_y\n",
    "            pred_boxes[..., 2] = torch.exp(w) * anchor_w\n",
    "            pred_boxes[..., 3] = torch.exp(h) * anchor_h\n",
    "            \n",
    "            # Process targets\n",
    "            target_mask, obj_mask, noobj_mask, tx, ty, tw, th, tconf, tcls = self.build_targets(\n",
    "                pred_boxes, targets[i], scaled_anchors, grid_size\n",
    "            )\n",
    "            \n",
    "            # Calculate losses\n",
    "            loss_x = self.mse(x[obj_mask], tx[obj_mask])\n",
    "            loss_y = self.mse(y[obj_mask], ty[obj_mask])\n",
    "            loss_w = self.mse(w[obj_mask], tw[obj_mask])\n",
    "            loss_h = self.mse(h[obj_mask], th[obj_mask])\n",
    "            loss_conf_obj = self.bce(pred_conf[obj_mask], tconf[obj_mask])\n",
    "            loss_conf_noobj = self.bce(pred_conf[noobj_mask], tconf[noobj_mask])\n",
    "            loss_conf = self.obj_scale * loss_conf_obj + self.noobj_scale * loss_conf_noobj\n",
    "            loss_cls = self.bce(pred_cls[obj_mask], tcls[obj_mask])\n",
    "            \n",
    "            total_loss += loss_x + loss_y + loss_w + loss_h + loss_conf + loss_cls\n",
    "            \n",
    "        return total_loss\n",
    "\n",
    "def build_targets(self, pred_boxes, target, anchors, grid_size):\n",
    "    \n",
    "    batch_size = pred_boxes.size(0)\n",
    "    num_anchors = len(anchors)\n",
    "    \n",
    "    # Initialize output tensors\n",
    "    obj_mask = torch.zeros(batch_size, num_anchors, grid_size, grid_size, dtype=torch.bool, device=pred_boxes.device)\n",
    "    noobj_mask = torch.ones(batch_size, num_anchors, grid_size, grid_size, dtype=torch.bool, device=pred_boxes.device)\n",
    "    target_mask = torch.zeros(batch_size, num_anchors, grid_size, grid_size, dtype=torch.bool, device=pred_boxes.device)\n",
    "    \n",
    "    tx = torch.zeros(batch_size, num_anchors, grid_size, grid_size, dtype=torch.float, device=pred_boxes.device)\n",
    "    ty = torch.zeros(batch_size, num_anchors, grid_size, grid_size, dtype=torch.float, device=pred_boxes.device)\n",
    "    tw = torch.zeros(batch_size, num_anchors, grid_size, grid_size, dtype=torch.float, device=pred_boxes.device)\n",
    "    th = torch.zeros(batch_size, num_anchors, grid_size, grid_size, dtype=torch.float, device=pred_boxes.device)\n",
    "    tconf = torch.zeros(batch_size, num_anchors, grid_size, grid_size, dtype=torch.float, device=pred_boxes.device)\n",
    "    tcls = torch.zeros(batch_size, num_anchors, grid_size, grid_size, self.num_classes, dtype=torch.float, device=pred_boxes.device)\n",
    "    \n",
    "    if len(target) == 0:\n",
    "        return target_mask, obj_mask, noobj_mask, tx, ty, tw, th, tconf, tcls\n",
    "    \n",
    "    # Convert anchors to tensor\n",
    "    anchors = torch.tensor(anchors, device=pred_boxes.device)\n",
    "    \n",
    "    # For each ground truth box\n",
    "    for target_idx in range(len(target)):\n",
    "        # Get batch index, class, and box coordinates\n",
    "        batch_idx = int(target[target_idx, 0])\n",
    "        class_idx = int(target[target_idx, 1])\n",
    "        \n",
    "        # Get ground truth box coordinates (normalized)\n",
    "        gx = target[target_idx, 2] * grid_size  # center x\n",
    "        gy = target[target_idx, 3] * grid_size  # center y\n",
    "        gw = target[target_idx, 4] * grid_size  # width\n",
    "        gh = target[target_idx, 5] * grid_size  # height\n",
    "        \n",
    "        # Get grid cell coordinates\n",
    "        gi = int(gx)\n",
    "        gj = int(gy)\n",
    "        \n",
    "        # Get ground truth box in anchor format\n",
    "        gt_box = torch.tensor([0, 0, gw, gh], device=pred_boxes.device)\n",
    "        \n",
    "        # Calculate IoU between ground truth and anchor boxes\n",
    "        anchor_boxes = torch.cat((torch.zeros(num_anchors, 2, device=pred_boxes.device), anchors), 1)\n",
    "        anchor_ious = bbox_iou(gt_box.unsqueeze(0), anchor_boxes, x1y1x2y2=False)\n",
    "        \n",
    "        # Find the best matching anchor box\n",
    "        best_anchor_idx = anchor_ious.argmax()\n",
    "        \n",
    "        # Only process if the best IoU is above a threshold (usually 0.3-0.5)\n",
    "        if anchor_ious[best_anchor_idx] > 0.3:\n",
    "            # Set masks\n",
    "            obj_mask[batch_idx, best_anchor_idx, gj, gi] = True\n",
    "            noobj_mask[batch_idx, best_anchor_idx, gj, gi] = False\n",
    "            target_mask[batch_idx, best_anchor_idx, gj, gi] = True\n",
    "            \n",
    "            # Set target values\n",
    "            tx[batch_idx, best_anchor_idx, gj, gi] = gx - gi  # x offset\n",
    "            ty[batch_idx, best_anchor_idx, gj, gi] = gy - gj  # y offset\n",
    "            \n",
    "            # Width and height targets (log space)\n",
    "            tw[batch_idx, best_anchor_idx, gj, gi] = torch.log(gw / anchors[best_anchor_idx, 0] + 1e-16)\n",
    "            th[batch_idx, best_anchor_idx, gj, gi] = torch.log(gh / anchors[best_anchor_idx, 1] + 1e-16)\n",
    "            \n",
    "            # Confidence and class targets\n",
    "            tconf[batch_idx, best_anchor_idx, gj, gi] = 1\n",
    "            tcls[batch_idx, best_anchor_idx, gj, gi, class_idx] = 1\n",
    "    \n",
    "    return target_mask, obj_mask, noobj_mask, tx, ty, tw, th, tconf, tcls\n",
    "    \n",
    "    def bbox_iou(box1, box2, x1y1x2y2=True):\n",
    "        if not x1y1x2y2:\n",
    "            # Transform from center and width to exact coordinates\n",
    "            b1_x1, b1_x2 = box1[:, 0] - box1[:, 2] / 2, box1[:, 0] + box1[:, 2] / 2\n",
    "            b1_y1, b1_y2 = box1[:, 1] - box1[:, 3] / 2, box1[:, 1] + box1[:, 3] / 2\n",
    "            b2_x1, b2_x2 = box2[:, 0] - box2[:, 2] / 2, box2[:, 0] + box2[:, 2] / 2\n",
    "            b2_y1, b2_y2 = box2[:, 1] - box2[:, 3] / 2, box2[:, 1] + box2[:, 3] / 2\n",
    "        else:\n",
    "            b1_x1, b1_y1, b1_x2, b1_y2 = box1[:, 0], box1[:, 1], box1[:, 2], box1[:, 3]\n",
    "            b2_x1, b2_y1, b2_x2, b2_y2 = box2[:, 0], box2[:, 1], box2[:, 2], box2[:, 3]\n",
    "        \n",
    "        # Get the coordinates of the intersection rectangle\n",
    "        inter_rect_x1 = torch.max(b1_x1.unsqueeze(1), b2_x1.unsqueeze(0))\n",
    "        inter_rect_y1 = torch.max(b1_y1.unsqueeze(1), b2_y1.unsqueeze(0))\n",
    "        inter_rect_x2 = torch.min(b1_x2.unsqueeze(1), b2_x2.unsqueeze(0))\n",
    "        inter_rect_y2 = torch.min(b1_y2.unsqueeze(1), b2_y2.unsqueeze(0))\n",
    "        \n",
    "        # Intersection area\n",
    "        inter_area = torch.clamp(inter_rect_x2 - inter_rect_x1 + 1, min=0) * \\\n",
    "                     torch.clamp(inter_rect_y2 - inter_rect_y1 + 1, min=0)\n",
    "        \n",
    "        # Union Area\n",
    "        b1_area = (b1_x2 - b1_x1 + 1) * (b1_y2 - b1_y1 + 1)\n",
    "        b2_area = (b2_x2 - b2_x1 + 1) * (b2_y2 - b2_y1 + 1)\n",
    "        union_area = b1_area.unsqueeze(1) + b2_area.unsqueeze(0) - inter_area\n",
    "        \n",
    "        return inter_area / union_area\n",
    "\n",
    "class COCODataset(Dataset):\n",
    "    def __init__(self, root_dir, ann_file, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.coco = COCO(ann_file)\n",
    "        self.ids = list(self.coco.imgs.keys())\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.ids[idx]\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "        annotations = self.coco.loadAnns(ann_ids)\n",
    "        \n",
    "        # Load image\n",
    "        img_info = self.coco.loadImgs(img_id)[0]\n",
    "        img_path = os.path.join(self.root_dir, img_info['file_name'])\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # Get bounding boxes and labels\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for ann in annotations:\n",
    "            bbox = ann['bbox']  # [x, y, width, height]\n",
    "            # Convert to YOLO format [center_x, center_y, width, height]\n",
    "            x_center = (bbox[0] + bbox[2]/2) / img_info['width']\n",
    "            y_center = (bbox[1] + bbox[3]/2) / img_info['height']\n",
    "            width = bbox[2] / img_info['width']\n",
    "            height = bbox[3] / img_info['height']\n",
    "            boxes.append([x_center, y_center, width, height])\n",
    "            labels.append(ann['category_id'])\n",
    "        \n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.tensor(labels, dtype=torch.long)\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img, boxes, labels\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_idx, (images, boxes, labels) in enumerate(dataloader):\n",
    "        images = images.to(device)\n",
    "        boxes = boxes.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(images)\n",
    "        loss = loss_fn(predictions, [boxes, labels])\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Batch [{batch_idx}/{len(dataloader)}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, boxes, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            boxes = boxes.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            predictions = model(images)\n",
    "            loss = loss_fn(predictions, [boxes, labels])\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def main():\n",
    "    # Hyperparameters\n",
    "    BATCH_SIZE = 8\n",
    "    LEARNING_RATE = 1e-4\n",
    "    NUM_EPOCHS = 100\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Transform\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Dataset and DataLoader\n",
    "    train_dataset = COCODataset(\n",
    "        root_dir='path/to/coco/train2017',\n",
    "        ann_file='path/to/coco/annotations/instances_train2017.json',\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    val_dataset = COCODataset(\n",
    "        root_dir='path/to/coco/val2017',\n",
    "        ann_file='path/to/coco/annotations/instances_val2017.json',\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "        collate_fn=collate_fn  # Need to implement this\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    # Model, optimizer, and loss\n",
    "    model = YOLOv3(num_classes=COCO_CLASSES).to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    loss_fn = YOLOLoss(ANCHORS, COCO_CLASSES, IMAGE_SIZE)\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', patience=3, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f'\\nEpoch {epoch+1}/{NUM_EPOCHS}')\n",
    "        \n",
    "        # Train\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, loss_fn, DEVICE)\n",
    "        print(f'Training Loss: {train_loss:.4f}')\n",
    "        \n",
    "        # Validate\n",
    "        val_loss = evaluate(model, val_loader, loss_fn, DEVICE)\n",
    "        print(f'Validation Loss: {val_loss:.4f}')\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "            }, 'best_model.pth')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
